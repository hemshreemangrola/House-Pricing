{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regression\n",
    "\n",
    "Neural Networks are first introduced with image recognition but in general neural networks are used for clustering through unsupervised learning, classification and regression through supervised learning. It not only helps to group unlabeled data, categorize labeled data but also predict continuous values.\n",
    "\n",
    "For neural network regression, we have to scale the values to a range of 0 to 1. It is similar to Standard scalar with a specified range in Machine Learning.\n",
    "\n",
    "In classification model we swap the softmax layer at the end with a linear layer, and swap the cross-entropy loss with root mean squared error or mean squared error.<br><br>\n",
    "\n",
    "### Initialization factors:\n",
    "\n",
    "There are a number of important, and sometimes subtle, choices that need to be made when building and training a neural network. You have to decide how many layers to have, which optimization algorithm is best suited for the network, etc. The initial weight plays a very important role on convergence rate and the quality of the network. <br><br>\n",
    "\n",
    "### Major types of normalization: \n",
    "\n",
    "1. ReLU: ReLU Layer applies an elementwise activation function max(0,x), which turns negative values to zeros (thresholding at zero). This layer does not change the size of the volume and there are no hyperparameters.\n",
    "\n",
    "2. Tanh: The range of the tanh function is [-1,1]\n",
    "\n",
    "3. Sigmoid: The range of sigmoid function is [0,1]<br><br>\n",
    "\n",
    "\n",
    "### Regularization:\n",
    "\n",
    "Dropout forces an artificial neural network to learn multiple independent representations of the same data by alternately randomly disabling neurons in the learning phase. Dropout is a vital feature in almost every state-of-the-art neural network implementation. To perform dropout on a layer, you randomly set some of the layer's values to 0 during forward propagation.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baby Panda\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Baby Panda\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#import the libraries\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data \n",
    "data = pd.read_csv('modified_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the features for neural network regression, here the predictors are all the selected variables according to the hypothesis\n",
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "#Selecting Target variable, which is saleprice\n",
    "y = data['SalePrice'].as_matrix()\n",
    "del data['SalePrice']\n",
    "\n",
    "\n",
    "#selecting the predictors, all the variables except saleprice\n",
    "X = data.as_matrix()\n",
    "\n",
    "#Dividing the data to test and train for regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural network regression, we have to scale the values to a range of 0 to 1. It is similar to Standard scalar with a specified range in Machine Learning.Performing a scaling on test set is equally important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping y variables\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Like standard scalar for simple ML algorithms, in Neural networks we have to convert the values to a range from 0 to 1\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scaling the training variables\n",
    "X_scaled_training = X_scaler.fit_transform(X_train)\n",
    "Y_scaled_training = Y_scaler.fit_transform(y_train)\n",
    "\n",
    "# Scaling the test variables\n",
    "X_scaled_testing = X_scaler.transform(X_test)\n",
    "Y_scaled_testing = Y_scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "learning_rate = 0.003\n",
    "training_epochs = 1500\n",
    "batch_size = 10\n",
    "dropout_rate = 0.7\n",
    "total_len = X_train.shape[0]\n",
    "input_nodes = X_train.shape[1]\n",
    "output_nodes = 1\n",
    "\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "l1_nodes = 150\n",
    "l2_nodes = 250\n",
    "l3_nodes = 300\n",
    "l4_nodes = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 70337.1364\n",
      "----------------------------\n",
      "Epoch: 0101 cost= 30001.4291\n",
      "----------------------------\n",
      "Epoch: 0201 cost= 27310.2516\n",
      "----------------------------\n",
      "Epoch: 0301 cost= 26125.4072\n",
      "----------------------------\n",
      "Epoch: 0401 cost= 26856.8483\n",
      "----------------------------\n",
      "Epoch: 0501 cost= 25141.2505\n",
      "----------------------------\n",
      "Epoch: 0601 cost= 23340.3170\n",
      "----------------------------\n",
      "Epoch: 0701 cost= 21846.9591\n",
      "----------------------------\n",
      "Epoch: 0801 cost= 22470.4396\n",
      "----------------------------\n",
      "Epoch: 0901 cost= 21747.8114\n",
      "----------------------------\n",
      "Epoch: 1001 cost= 21956.1568\n",
      "----------------------------\n",
      "Epoch: 1101 cost= 21446.6551\n",
      "----------------------------\n",
      "Epoch: 1201 cost= 21406.0626\n",
      "----------------------------\n",
      "Epoch: 1301 cost= 23782.5771\n",
      "----------------------------\n",
      "Epoch: 1401 cost= 20922.3198\n",
      "----------------------------\n",
      "----------------------------\n",
      "Accuracy: 35735.082\n"
     ]
    }
   ],
   "source": [
    "# define placeholders\n",
    "x = tf.placeholder(\"float\", [None, 53])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def model(x, weights, biases):\n",
    "    with tf.name_scope('neural_network'):\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['weight1']), biases['biase1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['weight2']), biases['biase2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['weight3']), biases['biase3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['weight4']), biases['biase4'])\n",
    "        layer_4 = tf.nn.relu(layer_4)\n",
    "        out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "# weights and bias for each layer\n",
    "with tf.name_scope('weights_biases'):\n",
    "    weights = {\n",
    "        'weight1': tf.Variable(tf.random_normal([input_nodes, l1_nodes], 0, 0.17)),\n",
    "        'weight2': tf.Variable(tf.random_normal([l1_nodes, l2_nodes], 0, 0.007)),\n",
    "        'weight3': tf.Variable(tf.random_normal([l2_nodes, l3_nodes], 0, 0.007)),\n",
    "        'weight4': tf.Variable(tf.random_normal([l3_nodes, l4_nodes], 0, 0.007)),\n",
    "        'out': tf.Variable(tf.random_normal([l4_nodes, output_nodes], 0, 0.007))\n",
    "    }\n",
    "    biases = {\n",
    "        'biase1': tf.Variable(tf.random_normal([l1_nodes], 0, 0.03)),\n",
    "        'biase2': tf.Variable(tf.random_normal([l2_nodes], 0, 0.03)),\n",
    "        'biase3': tf.Variable(tf.random_normal([l3_nodes], 0, 0.003)),\n",
    "        'biase4': tf.Variable(tf.random_normal([l4_nodes], 0, 0.03)),\n",
    "        'out': tf.Variable(tf.random_normal([output_nodes], 0, 0.03))\n",
    "    }\n",
    "\n",
    "# Model definition\n",
    "with tf.name_scope(\"train\"):\n",
    "    y_pred = model(x, weights, biases)\n",
    "    \n",
    "    #cost calculates the rmse value\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_pred, y))))\n",
    "    \n",
    "    #Adam optimizer to reduce the rmse value\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train dataset\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(total_len/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch-1):\n",
    "            batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c, p = sess.run([optimizer, cost, y_pred], feed_dict={x: batch_x,y: batch_y})\n",
    "                                                          \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        # sample prediction\n",
    "        label_value = batch_y\n",
    "        estimate = p\n",
    "        err = label_value-estimate\n",
    "        #print (\"num batch:\", total_batch)\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % 100 == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.4f}\".format(avg_cost))\n",
    "            print (\"----------------------------\")\n",
    "           \n",
    "\n",
    "    # Test model\n",
    "    accuracy = sess.run(cost, feed_dict={x: X_test, y: y_test})\n",
    "    predicted_vals = sess.run(y_pred, feed_dict={x: X_test})\n",
    "    print (\"----------------------------\")\n",
    "    print (\"Accuracy:\", accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to [Kaggle kernel](https://github.com/hmangrola/Predicting-House-Prices-Ames-Iowa/blob/master/Kaggle_Kernel.ipynb) notebook\n",
    "\n",
    "This kernel is licensed (license can be found on github repository)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
